%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{color}
\usepackage{psfrag}
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{listings}
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Other Settings

%%%%%%%%%%%%%%%%%%%%%%%%%% Page Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\geometry{a4paper}

%%%%%%%%%%%%%%%%%%%%%%%%%% Define some useful colors %%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{ocre}{RGB}{243,102,25}
\definecolor{mygray}{RGB}{243,243,244}
\definecolor{deepGreen}{RGB}{26,111,0}
\definecolor{shallowGreen}{RGB}{235,255,255}
\definecolor{deepBlue}{RGB}{61,124,222}
\definecolor{shallowBlue}{RGB}{235,249,255}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Define an orangebox command %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\orangebox[1]{\fcolorbox{ocre}{mygray}{\hspace{1em}#1\hspace{1em}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%% English Environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheoremstyle{mytheoremstyle}{3pt}{3pt}{\normalfont}{0cm}{\rmfamily\bfseries}{}{1em}{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{,--,#3}}
\newtheoremstyle{myproblemstyle}{3pt}{3pt}{\normalfont}{0cm}{\rmfamily\bfseries}{}{1em}{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{,--,#3}}
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[linewidth=1pt,backgroundcolor=shallowGreen,linecolor=deepGreen,leftmargin=0pt,innerleftmargin=20pt,innerrightmargin=20pt,]{theorem}{Theorem}[section]

\theoremstyle{mytheoremstyle}
\newmdtheoremenv[linewidth=1pt,backgroundcolor=shallowBlue,linecolor=deepBlue,leftmargin=0pt,innerleftmargin=20pt,innerrightmargin=20pt,]{definition}{Definition}[section]
\theoremstyle{myproblemstyle}
\newmdtheoremenv[linecolor=black,leftmargin=0pt,innerleftmargin=10pt,innerrightmargin=10pt,]{problem}{Problem}[section]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Plotting Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepgfplotslibrary{colorbrewer}
\pgfplotsset{width=8cm,compat=1.9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}




\title{
	AI Notes

}
\author{
	Viv Sedov --- \texttt{viv.sedov@hotmail.com}
}
\maketitle

\tableofcontents

\newpage



\section{Propositional\_logic}

\subsection{Simple Operations}
When covering this : there are simple operations that you should know about :

Simple Operation Not:

\[\begin{array}{c | c}
		p & \neg p \\
		\hline
		1 & 0      \\
		0 & 1
	\end{array}
\]

Such that in this case it is the opposite given value $\neg p $ would be the direct opposite of the given Value of P .

For example say that you have the following

\[\begin{array}{c c | c}
		p & q & p \land q \\
		\hline
		1 & 1 & 1         \\
		1 & 0 & 0         \\
		0 & 1 & 0         \\
		0 & 0 & 0
	\end{array}\]
In this example, we are stating the following command : $\neg(14 > 6)$ is false , we create this truth table to prove if that is true or not .

$P \land Q $ is true $\iff$ p and q are true

Simple $\lor$ - OR


\[\begin{array}{c c | c}
		p & q & p \lor q \\
		\hline
		1 & 1 & 1        \\
		1 & 0 & 1        \\
		0 & 1 & 1        \\
		0 & 0 & 0        \\
	\end{array}\]

In this example above , for this to hold true, atleast one of the values would have to have a one it to hold true, this is known as an \textit{Inclusive} or as in you can say the following and it would make sense:
\textbf{I will go to the shops $\lor$ i will go to the coast}\\

\\

Simple Operation $\implies$
This is where if something is true the other must be true , or where you given an equivalent pointer to if p then q


\[\begin{array}{ c c | c }
		p & q & p \implies q \\
		\hline
		1 & 1 & 1            \\
		1 & 0 & 0            \\
		0 & 1 & 1            \\
		1 & 1 & 1            \\
	\end{array}\]


With the above example this is not cause and effect , there is a reason for why this occours , and that is that there is a pointer such that it acts like an if statment, with the following code shown below :

\begin{lstlisting}
foo = True
if foo:
    return 1
else:
    return 0
\end{lstlisting}

The code above is rather simple , but shows that if something is true , then you would have some sort of value expression , or some pointer that would return if it is correct or not .

In the weird scenario of f and t , where if f is false it implies that q is true , that is because  the q value is true , meaning that it would hold , a little trick for this one is that for what ever the second value is , if it is true , it will hold , if both are false , then it will true , though if one is true and the other is false , it will not hold .\\

Logically equivalent $\iff$ this can also be seen as if and only if or iff \\
Here is an example of the logical truth table behind this
\[\begin{array}{c c | c }
		p & q & p \iff q \\
		\hline
		1 & 1 & 1        \\
		1 & 0 & 0        \\
		0 & 1 & 0        \\
		0 & 0 & 1        \\
	\end{array}\]

With this , Where if something is true then the other must be true or if its false then the other would have to be false , in this case you are seeing if these two values are the same .

Example Exersises of how this would all work :
\\
Given that :
\[p = Logic is fun for jane \\ \\
	q = david does not like cabbage \\ \\
	r = david eyes are blue \]\\

We can then further express all of this in a notational form .
\begin{itemize}
	\item $\neg p \land q$
	      This would imply the following truth table
	      \[\begin{array}{c c c | c }
			      \neg p & p & q & \neg p \land q \\
			      \hline
			      0      & 1 & 1 & 0              \\
			      1      & 0 & 1 & 1              \\
			      0      & 1 & 0 & 0              \\
			      1      & 0 & 0 & 0
		      \end{array}\]
	\item $\neg R \land \neg P \implies$  Both are not goint to be true such that you would get only one possible answer for this  .
\end{itemize}


Multiple truth table example :
show the following in a truth table :
\\
\[p \land (q \implies r) \]

\[\begin{array}{c c c | c | c }
		p & q & r & q \implies r & p \land (q \implies r) \\
		\hline
		1 & 1 & 1 & 1            & 1                      \\
		1 & 1 & 0 & 0            & 0                      \\
		1 & 0 & 1 & 1            & 1                      \\
		0 & 1 & 0 & 0            & 0                      \\
		0 & 1 & 1 & 1            & 0                      \\
		0 & 0 & 0 & 1            & 0                      \\
	\end{array}\]

Here is another example to understand how these tables would all work together
\\
\[(p \implies q) \land (q \implies p)\]
\[\begin{array}{c c c | c | c}
		p & q & q \implies p & p \implies q & (p \implies q) \land (q \implies p) \\
		\hline
		1 & 1 & 1            & 1            & 1                                   \\
		0 & 1 & 0            & 1            & 0                                   \\
		1 & 0 & 1            & 0            & 1                                   \\
		0 & 0 & 1            & 1            & 1                                   \\
	\end{array}\] \\

\\ \\


\begin{definition}
	Two propositions are equal if they have the same truth values , they are known as $ P \implies Q$ this is known as logically equivalent
\end{definition}

\begin{definition}
	A proposition is tautology if it is always true an example of this is $ P \lor \neg P$ this is always true

\end{definition}

\begin{definition}
	A proposition is a contradiction if it is always false for example $ P \land \neg P $ this is always false
\end{definition}

\begin{definition}
	A proposition is \textbf{Contingent} if it is neither Always true or false
\end{definition}

\\
\section{Disjunctive Normal Form}
A given formula is said to be a \textit{Disjunctive normal form} when it is an Or $\implies \lor$ this is known as \textbf{DNF} a function is conjunctive when it has an $ \land $ form , within their proposition logic .

\[(p \land \neg q \land r) \lor (\neg q \land \neg r) \lor q \]

Everytime a given formula is built , we would folow the rules of propositional calculus , and how for each conjunctive formula there should be a disjunctive formula as well .

\subsection{DNF of mini terms for truth tables}
\begin{itemize}
	\item For each row whose truth value is true , write doen for each of the prposition variables , of $ p_i $ in the formula of it self , either $P_i$ is true in row or $\neg P_i$ if false.
	\item Repeate the first pointer , for the truth table where the formula is true and write down the dijunction of the conjuctions .
\end{itemize}

What you will rsee is that those two values will equal up such that the result of the formula in DNF is the equivalent to the original formula .

\section{Conjunctive Normal Form}
A formula is said to be \textbf{Conjunctive Normal Form} when its conjunction is $ \land $ of disjunctive of $ \lor $ an example of this is shown below :
\[(\neg P \lor Q \lor R \lor \neg S) \land (P \lor Q) \land \neg S \land (Q \lor \neg R \lor S)\]

Every expression built up according to the rules of calc , and suchj that for each conjunctive formula there is a similar or an equivelent formual that can be written in disjunctive form .

\paragraph{Summary}
\item Conjunctive form , or in brackets , and on the outside
\item Disjunctive form,  And in the brackets and or on the outside
\newpage
\section{Logical Equivalences}
We can use this to obtain normal form , when we use the implication law to eliminate subproccess - when ever you have a doubel negation and demorgans to bring a $ \neg $ you what this value to be on the outside  : here are the sub process of how this can be done :


\[\neg\neg P \iff P \]
This rule is the double negation Law \\ \\

\[ \begin{array}{c}
		(P \lor Q) \iff (Q \lor P)   \\
		(P \land Q) \iff (Q \land P) \\
		(P \iff Q) \iff (Q \iff P)
	\end{array}\]
Commutaive laws where both values would have to equal towards each other  .


\[\begin{array}{c}
		((P \lor Q) \lor R) \iff (P \lor (Q \lor R))    \\
		((P \land Q) \land R) \iff (P \land(Q \land R)) \\
	\end{array}\]
This is the associative laws , where it is very similar to how they work in matricies in which they can equate towards each other . \\

\[\begin{array}{c}
		((P \lor Q) \land R) \iff (P \lor (Q \land R)) \\
		((P \land Q) \lor R) \iff (P \land(Q \lor R))  \\
	\end{array}\]
This lase is the distributive law , in which the given values would be changed within a DNF and CNF representation

\[\begin{array}{c}
		(P \lor P) \iff P \\
		(P \land P) \iff P
	\end{array}\]
Idempotent laws where the values of it self would always equal to it self no matter what .

\item Demorgans Law :

\[\begin{array}{c}
		\neg(P \lor Q) \iff (\neg P \land \neg Q) \\
		\neg(P \land Q) \iff (\neg P \lor \neg Q) \\
		(P \land Q) \iff \neg(\neg P \lor \neg Q) \\
		(P \lor Q) \iff \neg(\neg P \land \neg Q) \\
	\end{array}\]
Most times when you lookat demorgans law , you will notice that its very similar to the laws that have been stated above, but the thing that you want to note is that you will see that they are equal in some sense ,  where an or , is a direct link with Not  and And it self.


\item $\S$ Contrapositive Laws
\[\begin{array}{c}
		(P \implies Q) \iff (\neg Q \implies \neg P)
	\end{array} \]

implication that imply towards each other are contrapositive and hence you can switch out the given details of that infomation .

where If Q is an active reciever then P must be an active pointer is the same as stating if not p equates to Not q, in some sense you should understand how that would work .

\item $\S$  Implication
\[\begin{array}{c}
		(P \implies Q) \iff (\neg P \lor Q) \\
		(P \implies Q) \iff \neg(P \land \neg Q)
	\end{array}\]

\item  $\S$ Furhter implication
\[\begin{array}{c}
		(P \lor Q) \iff (\neg P \implies Q) \\
		(P \land Q) \iff \neg(\neg p \implies \neg Q)
	\end{array}\]
Thi sone is rather annyoing , but the principle of how this works is very intriguing , if you do prove this via proof table you will see that they are truly equivalent:
$P \lor Q$

\[\begin{array}{c c | c}
		p & q & p \lor q \\
		\hline
		1 & 1 & 1        \\
		1 & 0 & 1        \\
		0 & 1 & 1        \\
		0 & 0 & 0        \\
	\end{array}\]



this is the same as :
$(\neg P \implies Q) $


\[\begin{array}{c c c | c}
		P & Q & p \implies Q & (\neg P \implies Q) \\
		\hline
		1 & 1 & 1            & 1                   \\
		0 & 1 & 1            & 1                   \\
		1 & 0 & 0            & 1                   \\
		0 & 0 & 1            & 0                   \\
	\end{array}\]

If you look at the given tables above you will notice that indeed they are the same, a truth tabel may be long but they are very good at breaking down the given data that you have into something more readble .


\item Further Implies and equivalences

\[\begin{array}{c}
		((P \implies R) \land (Q \implies R)) \iff ((P \lor Q) \implies R)   \\
		((P \implies Q) \land (P \implies R)) \iff ((P \implies (Q \land R)) \\
	\end{array}\]
With this law you are using the given equivelences that are shown above with the disjunctive and conjunctive views ,but within an equivelence ratio


\item  $\S$ Exportion Law
\[\begin{array}{c}
		((P \land Q) \implies R) \iff (P \implies (Q \implies R))
	\end{array}\]

This one is a good one , Mainly because if anything that does imply to another pointer , you can show that they are all equal towards each other .



\item $\S$ Side Notes
Within the compound proposition $\neg(P \lor Q) \&  (\neg P \land \neg Q) $ they are the same , hence why when you look at the proof that is shown above you will see that they are the same .

When ever you look at equivelences you will notice that connectives $ \lor \land $ will allways suggest that $ P \lor Q \implies Q \lor P $

\newpage

\section{Formal definition }

\begin{definition}[Valid]
	An argument would be considered valid if and only if it takes a form that makes it impossible fro the premise to be true, in the sense that the conclusion is never going to be false. it is not possible to show it to be false in some sense.
	A formula is valid if and only if it is true under every understanding of its given argument, or its given schema, we can say it is valid if true holds for everything..
\end{definition}

\begin{definition}[Sound implies Valid]
	Valid allows us to imply Soundness

	We can say that if you have a valid argument, then you also have a sound statement.
	am argument would be considered sound if it is valid and all the premise is true.
\end{definition}

\begin{definition}
	Sound, in logic a premise or conclusion is said to be valid if it is true under every possible understanding of its given argument, or its given schema.
	\begin{center}
		\begin{tabular}{ c c c }
			1  & 2 & 1 \\
			\hline
			24 & 5 & 1 \\
			\hline
			7  & 8 & 1
		\end{tabular}
	\end{center}

	The example their would be both valid and sound
	\begin{math}
		\forall axiom \exists axiom \implies \vdash A
	\end{math}
	What this just means is that A consist of either an axiom or can get derived from an axiom set using only the rules of inference

	\textbf{\textit{To Dumb this down even more, if you have statement x and you want to prove statement y, you can only do so by breaking x down into smaller statements to see if you can prove and show that y exists}}
	\\
	\begin{math}
		\forall x \exists y \implies \vdash y  \\
		\forall x \exists y \implies \vdash x \land y \\
		\forall x \exists y \implies \vdash x \lor y \\
		\forall x \exists y \implies \vdash x \implies y \\
		\forall x \exists y \implies \vdash x \iff y \\
		\forall x \exists y \implies \vdash x \leftarrow y \\
		\forall x \exists y \implies \vdash x \rightarrow y \\
		\forall x \exists y \implies \vdash x \leftrightarrow y \\
	\end{math}

	Thing to note is that it does \textit{Not} mean that \textbf{A is satisfied} this is a deduction but if you want to show satisfaction you would have to show

	\begin{math}
		A \ \ indicates \ \ \forall Axiom A \\
		\forall A A(\models A) \implies True
	\end{math}

\end{definition}

\begin{theorem}[Validity of statement]
	Validity says nothing about whether or not any statement of the premises is true or not, it only says that the conclusion is true under every possible understanding of the premises.
	the key work their is Understanding of its own premises .

	Such that validity states that its more about the form of an argument than it being true of it self.
\end{theorem}

So we can say a argument is valid if it has the proper form . An argument can have the right form, but be false.
\\
\begin{tt}
	Daffy Duck is a duck \\
	All ducks are insects \\
	their for daffy duck is a insect \\
\end{tt}

Notice how these argument contain a form for \textit{if x is Y} but then you see that they are not true.
Notice However that if the premises \textbf{Were} True then the conclusion would also have to be true - this is a valid proof for validity. A valid argument needs not have true premises or a true conclusion.

\begin{theorem}[Sound requires a true premises]
	Sound logically implies that a statement is true, this is due to the fact that, when a statement is sound, it means that it has a true premises and a true conclusion, we can formally derive x from y using this factor.

	\textbf{\textit{Soundness}} Is an argument or a factor if it means the following arguments

	\begin{itemize}
		\item It is valid
		\item it has a true premises
	\end{itemize}
\end{theorem}

\begin{enumerate}
	\item  Sound requires both valid and to have a true premisies
	\item for all valid arguments if the premises are true then the conlusion must also be true
\end{enumerate}

Example :

\begin{enumerate}
	\item All rabits are mammals
	\item bugs bunny is a rabbit
	\item Therfore, Bugs bunny is a mammal
\end{enumerate}
In this argument we state that all of the premises are true, then the conclusion is true, so it is valid, and the premises are true, all rabbitss in fact are mammals and bugs bunny is a rabitt - so our conclusion makes sense.

\begin{definition}
	Completeness
	\begin{displaymath}
		\alpha \models \beta \implies \alpha \vdash \beta
	\end{displaymath}
	i.e if we can show something is true, then we can say that it is provable - we want to be able to prove all true statements, but you can also get false statements - such that you can prove both false and true Statements, such that if you end up proving false then your statement is no longer sound.
\end{definition}
\newpage

\begin{definition}
	Soundness
	\begin{displaymath}
		\alpha \vdash \beta \implies \alpha \models \beta
	\end{displaymath}
	If we have a formulation i.e xy = 10, then we want to be able to show that fact. We do not want a system where we start out with something true and dedice something to be false, if we know something we should prove with our current knoweldge of breaking something down, that given statement would hold, through infierence rules.

	However it is conceivable that even if our system is sound, it maybe incomplete, regarding what it can express hence why it requires to have a competes property to ensure that our given formulation of our proof would hold true.


\end{definition}


\subsection{Summary without the crap }
If your KB \textit{Knowledge base }Entails Q then all interpretations ( assigning true or false ) values to variables that would allow you to evaluate your knowledge to True, also evaluates to Q to true
\begin{math}
	KB \models Q
\end{math}
\item Entailment refers to how premises lead to a conclusion
reacall how m(b) is a subset of m(a)

\begin{align}

	\includegraphics[width=200px]{../img/2022-03-18-16-26-04.png}
\end{align}
Leads into

\begin{align}
	\includegraphics[width=200px]{../img/proofOfModel.png}
\end{align}

Inference is a procedure for deriving a new sentence ‚Äòp‚Äô from ‚ÄòKB‚Äô following some algorithm.
\begin{math}
	KB \vdash p
\end{math}
The inference algorithm is sound if it derives only sentences that are entailed by KB. The inference algorithm is complete if whatever can be entailed by KB can also be inferred from KB.
Basically, an inference is an informal and less reliable kind of entailment.

\begin{itemize}
	\item We have a kb
	\item We have some sentence S - query
	\item we want to prove S from our KB
	\item We say it is sound and complete if the space of model is finite within 2 pow n
\end{itemize}

\begin{align*}
	\includegraphics[width=200px]{../img/2022-03-18-16-45-07.png}
\end{align*}

\subsection{Equivalences and normal form }

\begin{definition}
	A sentance is valid if it is true in all models
	\begin{displaymath}
		True, \alpha \lor \neg \alpha , \alpha \implies \alpha , (\alpha \land (\alpha \implies \beta )) \implies \beta
	\end{displaymath}
	We can say that Validity is directctly connected through the inference of the deduction theorum
	\begin{displaymath}
		KB \models \alpha  \iff(KB* \implies \alpha )
	\end{displaymath}
\end{definition}


\section{Resolution algorithm}
\begin{itemize}
	\item input Kb and S
	\item Output true if
	      $KB \models S$ False otherwise
	\item  Initalise a list of clauses  CNF(KB and not S)
	      \begin{itemize}
		      \item for each pair of clauses C i and c h
		      \item R implies resolution of i and j
		      \item new resolution is made
	      \end{itemize}
	\item If clauses are new then return false
	\item if cluauses unifie each other return true
\end{itemize}

\newpage
\subsection{Resolution properties}
\begin{itemize}

	\item Resolution is Sound i.e it produces a sentence that are entailed by their orginal owner
	\item Resolution is complete - it is guarantee to establish entailment of the query for every finite time
	\item Completeness is based on the key theorem
	      \begin{theorem}
		      If a set of clauses is unsatisfiable, then the set of all clauses that can be obtained by the resolution contains an empty clause
	      \end{theorem}
	\item So in short, we can say the opposite of a resolute theorem is that if we cannot find the empty clause the query must be satisfiable.
\end{itemize}

We use resolution to check consistency of how KB holds
\begin{math}
	Human \implies Mortal\\
	Peter \implies Human
\end{math}
We convert that into CNF
\begin{math}
	\neg Human \lor Mortal\\
	\neg Peter \lor Human \\
\end{math}
We can then give our resolution to this being
\begin{math}
	\neg Peter \lor Mortal
\end{math}

\begin{align}
	\includegraphics[width=200px]{../img/2022-03-18-17-42-10.png}
\end{align}
Here in this image you can see how we can go from one proof, after another by linking them, to return if a valid argument will be true or false.



\subsection{Horn Form KB}
Horn Form is a clause or a form in which a logical inference problem is given :
Given : \\
\begin{itemize}
	\item A KB is a set of information
	\item A sentance $\alpha \implies theorem $
\end{itemize}

If a sentence is in KB, are restricted to some special form, some of the sound inference rules might be complete . In short its being able to convert from CNF form into implication rules
\\
\begin{math}
	(\alpha \lor \neg \beta) \land (\neg \alpha \lor \neg \gamma \lor \delta ) \\
	\iff \\
	(\beta \implies \alpha ) \land (\alpha \implies \gamma) \implies \delta)
\end{math}
This is known as horn form normal form.

\begin{itemize}
	\item Two inference rules that are sound and complete with respect to properistional symbols for kb in the hron normal form
	      \begin{itemize}
		      \item resolution on a positive unit
		      \item Modus pones
	      \end{itemize}
\end{itemize}

\begin{itemize}
	\item Have to be in conjuctive form \( \lor \ldots \lor  \ldots \lor  \ldots \)
	\item Would Follow the three basic principles
	      \begin{itemize}
		      \item fact
		      \item Goal
		      \item Rule
	      \end{itemize}
	      These are defined whith the following statement
	\item Rule :
	      \begin{math}
		      Man and Gel \implies Tall
	      \end{math}
	      Saying Gel men are tall
	      this can be counter proved with the following statement to prove completness
	      \begin{math}
		      KB \models \alpha \iff(KB* \land \neg \alpha) \text{not provable to be satisfiable i.e unsatisfiable }

	      \end{math}
	      You want to end up showing that thing you are trying to prove may or may not have any models
\end{itemize}
\section{Chaining}
$\implies$ Just look at the slides

\section{Total Summary of propositional logic}
\begin{theorem}
	Inference: process of deriving sentences entailed by the KB
	\begin{displaymath}
		KB \vdash_i \alpha
	\end{displaymath}
	is a sentance where \( \alpha  \) can be derived from KB in teh process of i
\end{theorem}

\textbf{\textit{For first-order logic there exists a sound and complete inference procedure
		- i.e. the procedure will answer any question whose answer follows from
		what is known by the KB.}}

\subsection{Formal Definitions}
\begin{itemize}
	\item \begin{definition}[Syntax]
		      \textit{Syntax}: formal structure of how a sentence is made
	      \end{definition}
	\item \begin{definition}[Semantics]
		      \textit{Symatics}: Truth of sentences with respect of the model
	      \end{definition}
	\item \begin{definition}[Entailment]
		      \( KB \models \alpha \iff true \in \mathbf{T}, \forall Models \ \ KB\)
	      \end{definition}
	\item \begin{definition}[Sound]
		      Derivations produced only from entailed sentances
	      \end{definition}

	\item \begin{definition}[Completeness]
		      Can you prove your derived formulation from - any derivations can produce the same theorm in return
	      \end{definition}
\end{itemize}

\subsection{Syntax}
\begin{itemize}
	\item \begin{definition}[Atomic]
		      \( \perp \) , \(\neg \perp \) Or P $\implies$ Propositional logic for an atom
	      \end{definition}
	\item  \begin{definition}[Negated Atomic]
		      \( \neg \) anything with that just means its negated
	      \end{definition}
	\item \begin{definition}[Conjuction]
		      \( \land \)
	      \end{definition}
	\item \begin{definition}[Disjunction]
		      \( \lor \)
	      \end{definition}


	      \end{definition}
\end{itemize}

\subsection{Equivalence Norms}
\begin{itemize}
	\item \begin{definition}[Equivalence]
		      \( \equiv \)
	      \end{definition}
	\item \begin{definition}[Implication]
		      \( \implies \)
	      \end{definition}
	\item \begin{definition}[Negated Implication]
		      \( \neg \)
	      \end{definition}
	\item \begin{definition}[Biconditional]
		      \( \iff \)
	      \end{definition}
\end{itemize}

\begin{itemize}
	\item Conjuctive:
	      Conjuction of disjunctions of literals
	      \begin{definition}[CNF]

		      \( (A \lor  \neg B) \land (B \lor  \neg C \lor \neg D)\)
	      \end{definition}
	\item Disjunctive:
	      Disjunction of conjuctions of literals
	      \begin{definition}[DNF]
		      \begin{math}
			      (A \land B) \lor (A \land \neg C)\lor (\neg A \land \neg D)
		      \end{math}

	      \end{definition}
\end{itemize}
\subsection{Validity and Satisfiability wrt to KB}
\begin{theorem}
	\begin{displaymath}
		KB \models \alpha \iff true \in \mathbf{T}\\
		Kb \models \alpha  \iff (KB* \implies \alpha ) \in \mathbf{T}
	\end{displaymath}
\end{theorem}
\textbf{\textit{Note}} KB is a set of information in which \( KB \implies \alpha  \) Is not a wellformed formula - we get this through combining everything in the KB into one big conjuction.
\subsubsection{CNTD Valid and Satisfiable}
\begin{itemize}
	\item \begin{definition}
		      A sentence is satisfiable if it is true in some model
		      \begin{displaymath}
			      \alpha  \lor  \beta  , \gamma
		      \end{displaymath}
		      we can prove this by proving that \( \alpha  \) and \( \beta  \) are true in some model and \( \gamma  \) is true in some model we can derive values using modus pones
	      \end{definition}
	\item \begin{definition}[Unsatisfiable]
		      A sentence is unsatisfiable if it is false in some model
		      \begin{displaymath}
			      \alpha  \land \neg \beta , \gamma
		      \end{displaymath}
	      \end{definition}
	\item Satisfiability is met only for the knolwedge base under the pretence that
	      \begin{math}
		      KB \models \alpha  \iff (Kb* \land  \neg \alpha)
	      \end{math}
	      which proving this will allow us to say this has no models and therefore is unsatisfiable

\end{itemize}



\subsection{Modus Ponens}
\begin{itemize}
	\item \begin{definition}[Modus Ponens]
		      \begin{math}
			      \( \forall \alpha \in KB \ \ \alpha \models \beta \ \ \beta \models \gamma \ \ \gamma \models \delta \ \ \delta \models \alpha \text{ Where } \\
			      \beta \in KB \ \ \gamma \in KB \ \ \delta \in KB \ \ \alpha \models \gamma \ \ \gamma \models \delta \ \ \delta \models \alpha \text{} \\
		      \end{math}
	      \end{definition}
\end{itemize}

\subsection{Chaining}
\subsubsection{Forward Chaining}
Forward chaining is the process of deriving a new sentence from a set of sentences that are already known.
\begin{theorem}
	\begin{displaymath}
		\begin{array}{l}
			\textbf{KB} \vdash_i \alpha   \\
			\textbf{KB} \vdash_i \beta    \\
			\textbf{KB} \vdash_i \gamma   \\
			\textbf{KB} \vdash_i \delta   \\
			\textbf{KB} \vdash_i \epsilon \\
		\end{array}
		\Rightarrow \alpha \models \beta \models \gamma \models \delta \models \epsilon
	\end{displaymath}
\end{theorem}

\begin{itemize}
	\item Forward chaining is a automatic process that can be used to derive new sentences from a set of known sentences.
	\item May do allot of dead work
\end{itemize}
\subsubsection{Backward Chaining}
Backward chaining is the process of deriving a new sentence from a set of sentences that are already known.
the principle is teh same as forward chaining but we start from the end of the sentence and work our way back to the beginning.

\subsection{Summary}
Propositional Logic does not have enough power...
We have this big kb and we have to go through each one to represent it. it is impracticle, forward and backward chaining are linear time, and are only \textit{Complete} if you are working for horn clauses , else it is rather hard to convey.


\section{First Order Logic}
Note : Just use implication over and
when you have
\begin{definition}
	Constant : variables : functions
	\begin{displaymath}
		Knows(x,arithmatic)
	\end{displaymath}
	^^ this is Atomic formula
	\begin{displaymath}
		Student(x) \implies  knows(x,arithmetic)
	\end{displaymath}
	^^ Connective, which is applies to a formula,
	Student knows arithmetic

	\begin{displaymath}
		\forall x Student(x) \implies Knows(x, arithmetic)
	\end{displaymath}
	All students know arithmetic



\end{definition}
\textbf{\textit{Overall a great way of dealing with this, is actually making a graph, to  convert the sentence into a graph like form, where you have different operations, and then check how they evaluate each other , in a sense this is a way how you convert propositional logic into first order logic.}}

\begin{definition}[Model in first order logic]
	A model \textit{w} in first order logic maps constant sysmbols to objects, similar to how connected graphs are all linked together

	\begin{math}
		w(alice) = \omega _1 , w(bob) = \omega _2, w(arithmetic) = \omega _3
	\end{math}

	\textit{Predicate symbols, to tuples of those objects}
	\begin{math}
		w(knows) = {(\omega _1, \omega _3), (\omega _2,\omega _3), \ldots }
	\end{math}





\end{definition}
\begin{math}
	\forall x \implies Y
\end{math}
this is sound and works but if you have or use and with for all, then you are stating that everything and everything would be x and y

\begin{math}
	\forall x \land  y(x)
\end{math}
see how that does not hold

\begin{math}
	\exists xp \implies y
\end{math}
this does not pair well

\begin{math}
	\exists  xP \land Y
\end{math}
this is better

\begin{itemize}
	\item And with Exist
	\item forall with Implies
\end{itemize}

\subsection{Examples}
\begin{itemize}
	\item Which sentance best represents someones's mother is someone's female parent
	      \begin{math}
		      \\
		      \exists x, \exists y (Mother(x,y) \land (Female(x) \land Parent(x,y)))
		      \qquad \\
		      \text{Or}
		      \\
		      \forall x \forall y (Mother(x,y) \iff (Female(x) \land Parent(x,y)))
		      \\
		      \text{Or} \\
		      \exists x \forall y(Mother(x,y) \implies (Female(x) \land Parent(x,y)))
	      \end{math}
	      This can be quote opiniated, wrt containement, but lets read them through
	      \textit{Its the first and second one }
	      \begin{itemize}
		      \item there exist some x and some y where there is a mother x ,y and that x is a female and that x y is a parent
		            what that kinda means is saying that there can be a parent that is not a mother, in short you can say that y and x can be a parent but does not implcity imply that it can be a mother
		            its a bit confusing but just go with that logic
	      \end{itemize}
\end{itemize}
\subsubsection{Quantifiers order}
The order between all given Quantifiers are not the same and can be swapped between the first order logic points
\begin{itemize}
	\item \( \forall x \forall y  \) is the same as \( \forall x \forall y \iff \forall y \forall x \)
	\item \( \exists x \exists y \)  is the same as \( \exists x \exists y \iff \exists y \exists x \)
	\item \( \forall x \exists y  \) is \textit{Not} the same as \( \forall y \exists x \;\not\!\!\!\implies \forall x \exists y\)
	\item \( \forall x \exists y Loves(x,y)\) Every x loves some y so \textit{Everyone in the world is loved by atleast one person }
	\item \( \exists x \forall y Loves(x,y) \)Some x loves every Y is not the same as every x loves some y this is \textit{There exist a person who loves everyone in the world} the reason for this is because when you have two \( \forall A \forall  B \) this is a pair, that you can sort through
\end{itemize}

\begin{itemize}
	\item Universal Quantifiers:
	      \begin{math}
		      \forall x p(x) \iff p(a) \land p(b) \land \\
		      \exists x P(x) \iff p(a) \lor p(b) \lor \\
	      \end{math}
	\item properties
	      \begin{math}
		      \neg \forall x P(x) \iff \exists x \neg P(x)\\
		      \text{Demorgans law with or applies within logic}
	      \end{math}
	\item Usefull to convert natural language sentences to  first order Sentences :
	      \begin{itemize}
		      \item Every student knows arithmetic
		      \item We can use \( \forall  \) \( \forall x student(x) \implies knows(x, arithmetic) \) But if you use and, then you are saying, something else, or you are implying something else.
	      \end{itemize}

\end{itemize}

\newpage
\subsubsection{Quantifier duality}
Duality is important as these quantifiers are reversable
So you can say something like this
\[
	\forall x \ \ likes(x, icecream) \equiv \neg \exists x \ \ \neg likes(x, icecram)\\
	\\
	\\
	\qquad
	\exists x \ \ Likes(c, brocolai) \equiv \neg \forall x \ \ \neg likes(x, brocolai) \]

\subsection{More Examples to work on }
\begin{itemize}
	\item \begin{math}
		      \forall x ,y(brother(x,y) \implies Sibling(x,y))
	      \end{math}
	      \\
	      This is saying, for every x, if brother(x,y) then that logiclly implies every brother is a sibling
	      \\
	      \begin{math}
		      \forall x y(Sibling(x,y) \iff Sibling(y,x))
	      \end{math}
	      \\
	      Every x there is a y where x and y and y and x will always be a sibling
	\item A first cousin, is a child of a parents sibling
	      \\
	      \begin{math}
		      \forall x, y(FirstCoursin(x,y) \implies \exists p, p' Parent(p,x) \land Sibling(P', p) \land Parent(P',x))
	      \end{math}
	      \\
	      for every x y , you have a first cousin under the condition that x and y is a firstcoursin for x to y and that there exist some p and p prime where parent of x is P and sibling of P prime is p and parent of y is P prime
\end{itemize}

\subsubsection{Wumpus world example }
\begin{itemize}
	\item Squares are breezy near a it : \textit{Diagnostic rule}, this is where infierence would come into play, where you infer \textit{Cause from effect}
	      \begin{math}
		      \forall y Breeezy(y) \implies  \exists x \text{ Pit}(x) \land Adjacent(x,y)

	      \end{math}
	\item Casual Rule : infern effect from cause
	      \begin{math}
		      \forall x, y \text{ Pit }(x) \land Adjacent(x,y) \implies Breezy(y)
	      \end{math}
	\item None of this is complete, the point is we done imply each other , though this can be built to become complete
	      \begin{math}
		      \forall y Breezy(y) \iff[\exists x \text{Pit}(x) \land  Adjacent(x,y)]
	      \end{math}



\end{itemize}

\section{Usefull notes from the quiz}
\begin{itemize}
	\item Information about the knowledge base
	      \begin{itemize}
		      \item If a model is true in the \textit{Real world} then any sentance derived from that given model is sound through inference, procedure, and hence is entailed within the real world
	      \end{itemize}
	\item How do you tell if an agent is true in the real world ?
	      \begin{itemize}
		      \item If the agent has sensors, that allow you to create the connection with the sentance and that the agents knoledge base is sound then it would be true within the real world
		      \item The ai agents learning ability generates general rules from expierence that the ai belives to be true, this is can be faliable, as this depends on how good the agent is at expressing its own information

	      \end{itemize}
	\item A satisfiable sentance is a sentance that is true in all models : \textit{True}
	\item Following statements are unstatisfiable
	      \begin{math}
		      (\alpha  \land  \beta ) \land (\neg \alpha  \land (\neg \alpha \implies \neg \beta ))
	      \end{math}
	      this will always resolve to zero
	      \\
	      \begin{math}
		      \Delta  \land  \neg \Delta
	      \end{math}
	      This will also result in zero, anything with contains a negative will never be satisifable
	\item Horn Clause examples
	      \begin{itemize}
		      \item \begin{math}
			            (\neg B_1 \lor \neg B_2 \ldots \neg B_n \lor C)
			            \text{Horn clause rule, everything is negative but one}
		            \end{math}
		      \item \begin{math}
			            B
		            \end{math}
		            This is true
		      \item \begin{math}
			            \neg (D_1 \lor  d_2 \lor  d_3)
			            \text{Everything is a negative ->  Goal state}
		            \end{math}

		      \item Proof facts
		            \begin{itemize}
			            \item to prove \( KB \models \alpha  \) is equivalent to showing that \( Kb* \land \neg \alpha  \) has no models
			            \item the expression \( Kb* \implies  \alpha  \) and \( KB \models \alpha  \) are connected through deduction theorem
		            \end{itemize}

	      \end{itemize}
	\item \textbf{\textit{Problems with Propositional_logic \( \implies  \)   does not support variables}}
	      Propositinal logic has an issue where it does not support variables, due to this, your statement would have to be very large

	      \\
	      this is true, propositional logic works through the factor that you work with values, and cant assign anything directicly as it propositional logic follows two main principles
	      \begin{itemize}
		      \item Declarative, pieces of syntax respond to facts
		      \item Compositional
		            meaning that \( p_{12} \land B_{11}\) is derived from me meaning of \( \text{p over 1 2 and b over 1 1 } \)
	      \end{itemize}


\end{itemize}

\subsection{Examples}
There is some course that every student has taken
\begin{math}
	\exists y Course(y) \land [\forall x Student(x) \implies Takes(x,y)]\\
\end{math}
Remember exist contains and, and then for all have a implies

\\
How about \\

Every even integer greater than 2 is the sum of two primes
\begin{math}
	\forall x even(x) \land greater(x,2) \implies  \exists y \exists z Same(x,sum(z,y)) \land prime(y) \land(prime(z))
\end{math}
Sometimes you have two primes, you can state that you are using two prims and they are EE values

If a student takes a course and the course covers a concept then the student knows that concept
\begin{itemize}
	\item if => universal quantification means you have a bunch of them
\end{itemize}

\subsection{First order logic proofs}
All human lectures are happy
\begin{math}
	\forall x (Lecture(x) \land Human(x) \implies  Happy(x))
\end{math}
\begin{itemize}
	\item clauses are a disjunctive form of literals that hae to be converted in most cases
	\item A definite clause is a clause where everything is negative but one
	\item A definite clause tends to be when you have not(..... and  .... and ....) or x
	      Then you can use implication to show that exists for the values that you are working with .
\end{itemize}

Susan bought everything that tony bought
\begin{math}
	(\forall x Bought(Tony, x) \implies  Bought(Suzan,x))
\end{math}


\subsubsection{Inferences in First Order Logic (proofs)}

How do you prove through your kB:

\begin{definition}[Subsitution]
	First order logic contains unification and Substitution, this is due to the fact that this logic principle contains Variables, similar to how Prolog would work
	\begin{math}
		\delta  = {\frac{x}{John}}
	\end{math}
	Given a sentence S and a substitution ùö´
	\begin{math}
		\\
		S = King(x) \\
		\delta = {{\frac{x}{John}}}\\
		S\delta  = King(John)
	\end{math}
\end{definition}

\begin{definition}[Universal instance]
	Every instance that we make when we unify some value, to let a given value poses over another in some sense
	Unification is the process of making a substitution, and this is done by unifying the two values .
	They use each other.

	\begin{math}
		\frac{\forall x \alpha }{\alpha 0}
	\end{math}
	For any variable v and ground term g if 0 is the subsitution of v/g then we can say the about

	Here you replacing values, v through g, so you are allowed to replace variables, think of how python lets you change the variable name
\end{definition}

In this example, what you are doing is replacing x, as you are replacing x with john, richard etc, if you know something about all possible vs, then you can use one
\begin{definition}[ground term ]
	A ground term is a term that contains no variables

	\begin{displaymath}
		\forall x (King(a) \land Greedy(x) \implies  Evil(x))\\
		\text{yields}\\
		\forall x(king(John) \land Greedy(John) \implies  Evil(John))
	\end{displaymath}
	Ground terms are easy to work with.

\end{definition}

\begin{definition}[Existential Instantiation]
	For any sentence \( \alpha   \)  and \( v \) and constant K that does not need to apear anywhere but the KB, then we can say the following

	\begin{align}
		\includegraphics[width=200px]{../img/2022-04-18-22-58-10.png}
	\end{align}

	C1 needs to be brand new and make sure that, it does not appear anywhere within the state, because we know that it would have to be on the head of the king, pretty much .

\end{definition}

\begin{itemize}
	\item If you have universal instantiation , then \textit{We can instantiate if we have an value from the qunatified formula - so we can just replace x with some name as a ground term }
	\item If you have universal Instantiation , then \textit{We can say, we can replace the quantifiable Value with  a new Name, so we can just get rid of it and give it a big Capital Boi to name it, shown in the image above}
\end{itemize}
\subsubsection{Unification}
How do you replace variables, Just look at the prolog notes i guess, as the principles would be the same

\begin{align}
	\includegraphics[width=200px]{../img/2022-04-18-23-09-26.png}
\end{align}

\begin{itemize}
	\item Say you know that you are trying to find two people, here we can replace John , x with x = Jane
	      Its a unification, where you replace x with jane, and then both formulas would become the same.
	\item For the second case, X needs to become Oj and Y would have to become John when you are  replacing values
\end{itemize}
\textit{Where tf did you get Likes from }

\begin{definition}[General Unifier]
	These are the unifiers that do not have any or add any constraints, when you want an ai system towards the best answers, and quickly, so you want the answer to be as simple and quickly as possible.

	So we want our unifiers to be as simple as possible
\end{definition}

\subsubsection{Unification Algo}
Given any two formulas \( \alpha  \beta  \)  there is a very fast way of unifying things together

\begin{math}
	UNIFY(\alpha  , \beta ) = 0 \iff \alpha 0 = \beta 0
\end{math}
Which checks whether alpha and betra can be unified, with the the least or general unifier within our db
for example if you have\\
\begin{math}
	p(x,y,f(z)) \\
	p(f(y), A, x) \\
\end{math}
we can unify the following

\begin{math}
	\frac{x}{f(a)} \\
	\frac{y}{A} \\
	\frac{z}{A}\\
\end{math}
\begin{align}
	\includegraphics[width=200px]{../img/2022-04-19-00-15-23.png}
\end{align}

\begin{itemize}
	\item Unification is a "pattern matching" procedure that takes two atomic sentences, called literals, as input, and returns "failure" if they do not match and a substitution list, Theta, if they do match. That is, unify(p,q) = Theta means subst(Theta, p) = subst(Theta, q) for two atomic sentences p and q.
	\item Theta is called the most general unifier (mgu)
	\item All variables in the given two literals are implicitly universally quantified
	\item To make literals match, replace (universally-quantified) variables by terms
\end{itemize}
\\
\begin{lstlisting}
procedure unify(p, q, theta)
  Scan p and q left-to-right and find the first corresponding
     terms where p and q "disagree"  ; where p and q not equal
  If there is no disagreement, return theta  ; success
  Let r and s be the terms in p and q, respectively,
     where disagreement first occurs
  If variable(r) then
     theta = union(theta, {r/s})
     unify(subst(theta, p), subst(theta, q), theta)
  else if variable(s) then
     theta = union(theta, {s/r})
     unify(subst(theta, p), subst(theta, q), theta)
  else return "failure"
end
\end{lstlisting}

\subsubsection{Forward Chaining Proof }
We can use forward chaining to prove that this factor holds

\begin{itemize}
	\item Sound and complete fore first order definite clauses
	\item may not terminate in general if \( \alpha   \) is not entailed
	\item You need entailment with definite clauses
	\item \textit{DataLog} = First Order Logic definite Clauses + no functions
	      They are an iterative functional value that you work of with .
\end{itemize}


\subsection{First Order Logic End Summery}
Say we have the following sentence
\begin{math}
	\text{Which Rich person Does Admam admire ? }\\
	admire(Admin, Bill) \\
	admire(Admin, Colin) \\
	admire(x,y) \leftarrow lectures(x), lectures(y) \\
	lecturer(Adam) \\
	lectureer(Dav) \\
	lectureer(Eric) \\
	rich(colin)\\
	rich(Eric) \\
	rich(Adam) \leftarrow rich(colin)\\
\end{math}
If we do backtracking we can do the following to find our solution
\begin{align}
	\includegraphics[width=300px]{../img/2022-04-19-15-30-43.png}
\end{align}
This is a nicer way of showing a proof to a given kb




\section{Temporal Reasoning}

\subsection{Classification of Ai temporal reasoning problems}
\begin{itemize}
	\item Prediction problem :
	      i.e project problem, given an inital state and casual rule, describing a domain, we want to derive the state of the world resulting from some given sequence of actions
	      Think of how a chess game would work, thiunk of a stack, when you are going through data .

	\item Planning problem :
	      Planning problem given a description of the inital state, how are you going to change to rules to get to your domain state, through what actions will allow you to get to that point ?

	      \begin{align}
		      \includegraphics[width=200px]{../img/2022-03-26-23-59-10.png}
	      \end{align}
	\item Explanation problem - given some casual rules describing a domain, we want to discover facts about our given state
	      \textit{You wake up in the morning and you head downstairs in the kitchen theres a plate on the table and a bowl with a little milk left in, you can then say by assumption that your housemake was awoke before you and already had their breakfast}

\end{itemize}
\subsection{Terminology}
\begin{enumerate}
	\item \begin{definition}[Fluents]
		      Where the Truth varies over a period of time, think of them as a boolean state variable,
		      Like when a person x lies, but sometimes they can state the truth, it would follow that logic

		      we can also call this :
		      \textit{Temporal Propositional statements}
	      \end{definition}
	\item \begin{definition}[Time instances and time periods]
		      In short, year month day, or what happend on that date, we state to be time instances
		      for example
		      Last year my university decided to do in person exams to F every CS student over this allows us to give an initiation of our time  which leads into what occoured
	      \end{definition}
\end{enumerate}

We generate new fluences when a state is changed, for example in the image above, when you have your start state, b [c,[a]] in case you see that every variable there is in flux, whihch would imply that we would have to move c from a, and then move b to a etc ... to get to our goal state

\subsection{Frame problem}
The frame problem is about finding a great way to handle non change .
\begin{itemize}
	\item Peristence or Intertia - the assumption that the facts are not affect by its actions - such that they would hold the same truth value after the action, as they had before hadn

	\item The root of the problem lies in the fact of what the domain would represent

	\item  For smaller domains, with just a small number of fluents, and action types, we can just write it out properly.
\end{itemize}
To most AI researchers, the frame problem is the challenge of representing the effects of action in logic without having to represent explicitly a large number of intuitively obvious non-effects

types of problems
\begin{enumerate}
	\item Qualification problem - exhuastivly specifying preconditions of an action
	\item Computational fram to verify what was inferred

	\item Ramification problem - problem of exhastively specifying the effects of an action - think of chess


\end{enumerate}
TODO: Write more about Frame problem, as im not 100 \% sure how this works

\subsection{Situational Calculus}
The situation calculus is a logic formalism designed for representing and reasoning about dynamical domains. It was first introduced by John McCarthy in 1963. The main version of the situational calculus that is presented in this article is based on that introduced by Ray Reiter in 1991. Wikipedia
\textit{Domain - tend to be in first order logic :}\\
\begin{itemize}
	\item Fluents bassicly the item you are trying to change
	\item Actions the function you are pushing on those fluents
	\item Situations like an instance of the current state
\end{itemize}

\begin{align}
	\includegraphics[width=200px]{../img/2022-03-27-00-44-10.png}
\end{align}
\begin{align}
	\includegraphics[width=200px]{../img/2022-03-27-00-46-27.png}
\end{align}

\begin{itemize}
	\item Remover and append operatiions within a fleucen
	\item Representation of actions as it stands does not check whether it is possible for a block to be moved from one place to another $\implies$ They dont check if something has been moved
	\item How can you improve cause and effect
\end{itemize}
\textit{Situation Calculus is trying to deal with the reasoning of the present }

We can use situational calculus to represent the current condition of a state.
\subsubsection{Situational Calc state Representation}
Situational Calc is defined through states

if you look at the image above we have multiple situations, from s1 to s3, with different fluents and actions that have been made upon some given variable
\begin{align}
	\includegraphics[width=200px]{../img/2022-03-27-02-05-12.png}
\end{align}

If you initial state was S, and then you have this functional operation, \( a_1 \) then you would evaluate to a function that would be \( do(a_1,S) \) , and this is a recursive functino or a recursive call that is made.
These Do values here are new states, are when an action state, where they would be a new situation after previous situation, in which that would lead to another state.

\subsubsection{Language}
Situational Calc, is a bassis that relies of the following language
\begin{itemize}
	\item Constant and variables : \textit{These are for fluents and actions } they are usally sorted .
	\item names For situations, basic functional values, where we have some function that would denote some action that is being done within that given time

	\item \textit{Holds(F,S)} Represents a fluent F is true in the situation S \\
	      What this means is when ever something i shold, it means it will contain to be true within that state .
	      \begin{align}
		      \includegraphics[width=200px]{../img/2022-04-18-21-12-51.png}
	      \end{align}
	      Main difference is that representation of fluents using hold is that we base an action, on the new solution ? ? ?? wtf is this ?

	      \\
	      Move(current_state || What is it on || Where does it go )

	      Here you would use a hold, to check if something is true or not, we use hold to show a fluent .
	      Which would mean that b on top of f is true in s0, both are valid way of  showing fluents

	\item \textbf{\textit{Main Idea }} \\

	      \begin{itemize}
		      \item What holes true after an action occurs
		      \item What remains the same after an action occurs
		      \item What changes after an action
		      \item What were the pre conditions for that action to take place - and was it derived from previous actions.
	      \end{itemize}

\end{itemize}
\begin{definition}[Language of situational calculus]
	Difference between situation and state \\
	\begin{itemize}
		\item A state is the set of fluents that are always true
		\item A Situation provides a history of transitions (actions) from initial state
		      So think of the recursive idea, where you held all the previous states in one go, its similar to that principle .
	\end{itemize}
\end{definition}
\subsubsection{Axioms}
Axioms are statements that are always true, and are used to define the language of the situation calculus.
They are preconditions that are specified when working with an action

\begin{enumerate}
	\item Preconditions effect - Axioms
	      A fluent p holds in the situation do (·µÖ, s) so long as P and execution of alpha in S makes P true, and the precondition of A if any are all true for S.
	      So in some sense, when we have a function, we have to make sure that it would hold true, on execution  such that it makes sense
	      \begin{align}
		      \includegraphics[width=200px]{../img/2022-04-18-21-31-06.png}
	      \end{align}

	      So in this image it looks confusing and your right, its way too much crap for what its worth
	      So in short, when we say x is on z after we move y to z
	      \\
	      \\
	      action(move y to z) means (x is on z )
	      \\
	      \\
	      So when we have on(x,z) we have to show, what actions were made, so move(x,y,z) right, the arror denotes the preconditions, to the movement of the value.
	      Such that x had to be on y so we can move from y,
	      and then we have to clear x and clear z i.e we have to check that nothing was on top of x and y

	      These are knowns as preconditions being created.


	\item Frame Axioms
	      Indicating A fluent P holds in situation, where S is not stop, i.a you have a continuous axiom of representation.

	      Basically When something does not change, and your action state is doing something else, and not effecting your current clear or on value that you are working with.
\end{enumerate}
\newpage

\section{Inductive Learning}
Inductive Learning is pretty much, decision trees, high level, symbolic approach to how properties would work .
\textbf{\textit{Legit if else statements : think of disjunctive normal form }}


\begin{definition}[Learning]
	Learn is defined through the following items that allow us to measure how much an item can learn, a good approach to think about is, maybe how much IQ a person has ?
	\begin{enumerate}
		\item E = Experience
		\item T = Task
		\item P = Performance
	\end{enumerate}

	The learning is measured such that Our experience is based with respect to our task and that how well have we learning, such that we base that off our performance.
\end{definition}
\subsection{Why is learning Key to this ? }
\begin{itemize}
	\item System is based on agents within the real world, all be it sensors, or agents that we have
	\item some cases we have to have decision making to improve our performance
	\item For environments that may not be fully known, some cases we have to work within respect to what the agent would learn
	      \begin{itemize}
		      \item Acting on its own
		            \teim making its own rules through pattern matching
		      \item Handle data from our agent and much more
	      \end{itemize}
\end{itemize}

As always learning, is a basis that allow us to increase our performance over time, learning improves performance, because it can learn through its conditions, goals and other items within our system or our KB
\subsection{Forms of Learning}
There are different forms of learning
\subsubsection{Unsupervised Learning}
\begin{definition}[Unsupervised Learning]
	Learn through patterns, within our data set. : There is no external application in which you can go out your way to help and say oh you can do this . \\
	You have zero feedback and you learn with respect to the input data, over a period of time.
	\\
	Examples :
	\\
	\begin{itemize}
		\item Self Learning Cars; trying to figure out what is good traffic or not.
	\end{itemize}

\end{definition}
\subsubsection{Reinforcement Learning}
\begin{definition}[Reinforcement Learning]
	this is where you learn from series of rewards and punishments, think of a monkey, whenthey learn, or when a baby learns, there are expierences that they have to go through to get to where they are now, and there is a similar pricniple to what happens here

	you have the Benefits and the downsides, which need to be in balance but only learn through telling the agent this is what oyu can do, this is what you cannot do:

	think of it like, when you are trying to teach someone to drive somewhere.

\end{definition}
\subsubsection{Supervised Learning}
\begin{definition}[Supervised Learning]
	Agent observes some examples, you give it a training set and it learns from that training set: good example is when you messed around with neural networks to lean about this basis right, recall when you messed around with the mnist dataset, you had to give it a network and self learning properties but first you had to give it a data set for it to work with right.
	exact same principle here

\end{definition}

\subsubsubsection{Inductive Learning}
\begin{definition}[Inductive Learning]
	Inductive learning is like human learning based on past experiences
	\begin{itemize}
		\item the closest way of an ai agent to learn is to do so from observations or giving it the information or telling it to do something with respect to its previous data
		\item Agent will use learning of a target function to predict the values of an attribute, in which you can approve or not, or have certain qualifiers, where you can say the following thing about the given time or information you have you induce information: i.e inductive
		      \begin{itemize}
			      \item Approve
			      \item not approved
			      \item high risk
			      \item low risk
		      \end{itemize}

	\end{itemize}
	\textbf{\textit{Induction:}} Is a way to show how we can compare to things to gather, and say here are two sets of information, deduce some information from it, so we can work with new information, you are creating new info in some sense.

	This is a variant of supervised learning
\end{definition}
So with supervised learning we have these tasks of inference: \textit{What data do we have to learn from }

\begin{itemize}
	\item Training data
	\item test set
	\item Some Idea of what we are looking for
\end{itemize}


\subsubsubsection{How does Inductive Learning work}

\begin{itemize}
	\item Inductive learning is the basis for Choosing the best hypothesis, i.e the best data that we can work with
	\item our \textit{h } Would have to be consistent, or that we know that there are no issues with our data set
	\item \textbf{\textit{Learning problem realisable}} If \( G(h) \in H  \)  then we can say the following
	      \begin{itemize}
		      \item H = is a set of all valid functions that we can work with, i.e hypothesis or the expressive value that we got
		      \item if our G(consist value h) is within our hypothesis than we say our Learning problem is realisable
	      \end{itemize}
\end{itemize}

\begin{definition}[Ockham Razer ]
	Prefer the simplest hypothesis, we can say it follows a reasonable policy, or work with, simplest value
\end{definition}


\subsection{Decision Trees}
\begin{definition}[Decision Trees]
	A Decision Tree is a tree, that provides the following attributes
	\begin{itemize}
		\item Attributes based classification : meaning that for each value within your tree base, you have these attributes or objects that you base it upon, a decision tree is like  a massive if elif else statement in some sense that you are always working with.
		\item For the time we can consider this as boolean values that you can work with
		      \begin{math}
			      \mathbf{T}  \mathbf{F}
		      \end{math}
	\end{itemize}
\end{definition}
\subsubsection{Hypothesis Space}
\begin{itemize}
	\item How many distinct decision trees with n boolean attributes
	      \begin{math}
		      2^{2^{n}}
	      \end{math}
	\item How many purely conjuctive hypotheses are there in this
	      \begin{math}
		      hungrry \land \neg Rain
	      \end{math}
	      In this case you have \( 3^{n} \)  distinct conductive forms
\end{itemize}
\begin{definition}[Entropy]
	A basic qunatity associated to a random variable and interpreted as average level of some information that you work with

	Entropy is the measurement of disorder or impurities in the information processed in machine learning. It determines how a decision tree chooses to split data.

	We can understand the term entropy with any simple example: flipping a coin. When we flip a coin, then there can be two outcomes. However, it is difficult to conclude what would be the exact outcome while flipping a coin because there is no direct relation between flipping a coin and its outcomes. There is a 50 percent  probability of both outcomes; then, in such scenarios, entropy would be high. This is the essence of entropy in machine learning.

	- you have god notes on this, use that for the entropy sutff
\end{definition}

Link to understanding this :\url{https://www.javatpoint.com/entropy-in-machine-learning}
The goal is to have the resulting decision tree as small as
possible (Occam‚Äôs Razor)
\item ‚Äì The main decision in the algorithm is the selection of the next
attribute to condition on.
\item  We want attributes that split the examples to sets that are
relatively pure in one label; this way we are closer to a leaf
node.
\item ‚Äì The most popular heuristics is based on information gain, originated
with the ID3 system of Quinlan.

\begin{math}
	-p_+ \log (p_+) - p_- \log (p_-)
\end{math}
\begin{itemize}
	\item \( p_+ \) is proportion of postive numbers
	\item \( P_- \) Is proprotion of negative examples in S
\end{itemize}
Overall we can say this

when \( p_i \) is the fracxtion of the examples
\begin{math}
	-\sum_{n=i} p_i \log (p_i)
\end{math}


\end{document}
